{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Machine Learnt Patterns in Rhodium-Catalysed Asymmteric Michael Addition Using Chiral Diene Ligands"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Welcome to the Jupyter Notebook version of the python code for training\n",
    "regression models, random forests, and neural networks to find relationships between diene ligands\n",
    "and the asymmetric Michael Addition they perform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first part of using python code is installing and calling the appropriate \"libraries\".\n",
    "Libraries are prewritten code that perform lots of work for us.\n",
    "This means less writing code from scratch.\n",
    "Libraries are maintained by different groups and can be downloaded as and when needed for your project.\n",
    "Different projects can use \"Environments\" so that you only use the libraries needed for each project.\n",
    "\n",
    "Here we are using Pandas, numpy, math, keras, sys, sklearn (aka sci-kit learn) and tensorflow.\n",
    "\n",
    "Balancedkfold is  our own bit of code in another python file, balancedkfold.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import keras\n",
    "from sys import exit\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import balancedkfold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the libraries loaded - or just parts of them - we now move into the main code.\n",
    "\n",
    "We define a few functions first that we will use in the main routine.\n",
    "\n",
    "\n",
    "We define a function called 'choose_model' that will use our best parameters held in the\n",
    "dictionary 'best_params' for running our model.\n",
    "\n",
    "Here we have 'commented out' - turned off - all models but Random Forest Regression.\n",
    "\n",
    "If there are no paramters given then the parameters will be searched for to give the best fit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def choose_model(best_params):\n",
    "    if best_params == None:\n",
    "        # return LinearRegression()\n",
    "        return RandomForestRegressor()\n",
    "        # return GradientBoostingRegressor()\n",
    "        # return SVR()\n",
    "\n",
    "    else:\n",
    "        # return LinearRegression()\n",
    "        return RandomForestRegressor(n_estimators=best_params[\"n_estimators\"], max_depth=best_params[\"max_depth\"], min_samples_leaf=best_params['min_samples_leaf'], min_samples_split=best_params['min_samples_split'])\n",
    "        # return GradientBoostingRegressor(loss = best_params['loss'], learning_rate=best_params['learning_rate'],n_estimators=best_params[\"n_estimators\"], max_depth=best_params[\"max_depth\"],min_samples_leaf=best_params['min_samples_leaf'], min_samples_split=best_params['min_samples_split'])\n",
    "        # return SVR()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here the function is choosing the data set to be used. Again those we are not using are commented out."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def choose_dataset():\n",
    "    return 'Cyclic'\n",
    "    # return 'Acyclic'\n",
    "    # return 'Combined'\n",
    "    # return 'Cyclohexenone'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function is for tuning the hyper parameters, if we call the function to do so."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def hyperparam_tune(X, y, model):\n",
    "    print(str(model))\n",
    "    if str(model) == 'RandomForestRegressor()':\n",
    "        hyperP = dict(n_estimators=[100, 300, 500, 800], max_depth=[None, 5, 8, 15, 25, 30],\n",
    "                      min_samples_split=[2, 5, 10, 15, 100],\n",
    "                      min_samples_leaf=[1, 2, 5, 10])\n",
    "\n",
    "    elif str(model) == 'GradientBoostingRegressor()':\n",
    "        hyperP = dict(loss=['ls'], learning_rate=[0.1, 0.2, 0.3],\n",
    "                      n_estimators=[100, 300, 500, 800], max_depth=[None, 5, 8, 15, 25, 30],\n",
    "                      min_samples_split=[2],\n",
    "                      min_samples_leaf=[1, 2])\n",
    "\n",
    "    elif str(model) == 'SVR()':\n",
    "        hyperP = dict(kernel=['linear', 'rbf', 'poly', 'sigmoid'], gamma=['scale', 'auto'], C=[0.1, 1, 5, 10],\n",
    "                      epsilon=[0.001, 0.01, 0.1, 1, 5])\n",
    "\n",
    "\n",
    "    gridF = GridSearchCV(model, hyperP, cv=3, verbose=1, n_jobs=-1)\n",
    "    bestP = gridF.fit(X, y)\n",
    "    print(bestP.best_params_)\n",
    "    return bestP.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we define the random seeds.\n",
    "\n",
    "Computers are pseudo random, so we need to initialise our random number generators"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "random_seeds = np.random.random_integers(0, high=1000, size=30)\n",
    "print(random_seeds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the 'list' of descriptor names. Basically the headings from our CSV file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The CSV file is read in using the pandas read_csv function, and the columns in our file\n",
    "that do not correspond to the descriptors we have defined are removed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "descriptors = ['LVR1', 'LVR2', 'LVR3', 'LVR4', 'LVR5', 'LVR6', 'LVR7', 'VB', 'ER1', 'ER2', 'ER3', 'ER4', 'ER5', 'ER6',\n",
    "               'ER7', 'SStoutR1', 'SStoutR2', 'SStoutR3', 'SStoutR4', '%top']\n",
    "\n",
    "data = pd.read_csv(choose_dataset()+'.csv')\n",
    "\n",
    "data = data.filter(descriptors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the data read in we also remove those rows containing any erroneous values\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#remove erroneous data\n",
    "data = data.dropna(axis=0)\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We split our data into target  'Y', and descriptors 'X'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "X = data.drop(['%top'], axis = 1)\n",
    "X = RobustScaler().fit_transform(np.array(X))\n",
    "y = data['%top']\n",
    "print(X)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we have predefined our best parameters as a 'dictionary'.\n",
    "The parameter searching function usage is commented out.\n",
    "\n",
    "We also define containers for storing the results of our training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#best_params = hyperparam_tune(X, y, choose_model(best_params=None)) #hyperparameter tuning completed on whole subset\n",
    "best_params = {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 500}\n",
    "r2_cv_scores = []\n",
    "rmse_cv_scores = []\n",
    "r2_val_scores = []\n",
    "rmse_val_scores = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the models now all set up and ready to run we can start the training\n",
    "\n",
    "We loop over our random seeds, as we want to try different starting positions\n",
    "in the search space for fitting the model.\n",
    "\n",
    "Each time the data is split randomly into test and validation data to prevent overfitting.\n",
    "\n",
    "K fold crossvalidation fitting is also performed to further ensure no over fitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(random_seeds)):\n",
    "    # split into training and validation sets, 9:1\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                  test_size=0.1, random_state=random_seeds[i])\n",
    "    X_train = np.array(X_train).astype('float64')\n",
    "    X_val = np.array(X_val).astype('float64')\n",
    "    y_train = np.array(y_train).astype('float64')\n",
    "    y_val = np.array(y_val).astype('float64')\n",
    "\n",
    "    # 5 fold CV on training set, repeated 3 times\n",
    "    for j in range(3):\n",
    "        kfold = balancedkfold.BalancedKFold(5, verbose=False)  # Kohavi, 1995, stratified KFold\n",
    "        for train, test in kfold.split(X_train, y_train):\n",
    "            model = choose_model(best_params)\n",
    "            model.fit(X_train[train], y_train[train])\n",
    "            predictions = model.predict(X_train[test]).reshape(1, -1)[0]\n",
    "\n",
    "            r2 = r2_score(y_train[test], predictions)\n",
    "            rmse = math.sqrt(mean_squared_error(predictions, y_train[test]))\n",
    "            r2_cv_scores.append(r2)\n",
    "            rmse_cv_scores.append(rmse)\n",
    "\n",
    "\n",
    "    # predict on validaiton set\n",
    "    model = choose_model(best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_val)\n",
    "    r2 = r2_score(y_val, predictions)\n",
    "    rmse = math.sqrt(mean_squared_error(predictions, y_val))\n",
    "    r2_val_scores.append(r2)\n",
    "    rmse_val_scores.append(rmse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The final model results are then return to the command line."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print('Model:',  model)\n",
    "print('Data Subset: ',  choose_dataset())\n",
    "print('Random Seeds: ', random_seeds, '\\n')\n",
    "print('Num CV Scores: ', len(r2_cv_scores))\n",
    "print('CV R2 Mean: ', np.mean(np.array(r2_cv_scores)), '+/-', np.std(np.array(r2_cv_scores)))\n",
    "print('CV RMSE Mean %: ', np.mean(np.array(rmse_cv_scores)), '+/-', np.std(np.array(rmse_cv_scores)), '\\n')\n",
    "print('Num Val Scores: ', len(r2_val_scores))\n",
    "print(r2_val_scores)\n",
    "print('Val r2 Mean: ', np.mean(np.array(r2_val_scores)), '+/-', np.std(np.array(r2_val_scores)))\n",
    "print('Val RMSE Mean %: ', np.mean(np.array(rmse_val_scores)), '+/-', np.std(np.array(rmse_val_scores)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}