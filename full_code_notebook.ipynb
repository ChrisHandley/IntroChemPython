{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Machine Learnt Patterns in Rhodium-Catalysed Asymmteric Michael Addition Using Chiral Diene Ligands"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Welcome to the Jupyter Notebook version of the python code for training\n",
    "regression models, random forests, and neural networks to find relationships between diene ligands\n",
    "and the asymmetric Michael Addition they perform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first part of using python code is installing and calling the appropriate \"libraries\".\n",
    "Libraries are prewritten code that perform lots of work for us.\n",
    "This means less writing code from scratch.\n",
    "Libraries are maintained by different groups and can be downloaded as and when needed for your project.\n",
    "Different projects can use \"Environments\" so that you only use the libraries needed for each project.\n",
    "\n",
    "Here we are using Pandas, numpy, math, keras, sys, sklearn (aka sci-kit learn) and tensorflow.\n",
    "\n",
    "Balancedkfold is  our own bit of code in another python file, balancedkfold.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import keras\n",
    "from sys import exit\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import balancedkfold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the libraries loaded - or just parts of them - we now move into the main code.\n",
    "\n",
    "We define a few functions first that we will use in the main routine.\n",
    "\n",
    "\n",
    "We define a function called 'choose_model' that will use our best parameters held in the\n",
    "dictionary 'best_params' for running our model.\n",
    "\n",
    "Here we have 'commented out' - turned off - all models but Random Forest Regression.\n",
    "\n",
    "If there are no paramters given then the parameters will be searched for to give the best fit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def choose_model(best_params):\n",
    "    if best_params == None:\n",
    "        # return LinearRegression()\n",
    "        return RandomForestRegressor()\n",
    "        # return GradientBoostingRegressor()\n",
    "        # return SVR()\n",
    "\n",
    "    else:\n",
    "        # return LinearRegression()\n",
    "        return RandomForestRegressor(n_estimators=best_params[\"n_estimators\"], max_depth=best_params[\"max_depth\"], min_samples_leaf=best_params['min_samples_leaf'], min_samples_split=best_params['min_samples_split'])\n",
    "        # return GradientBoostingRegressor(loss = best_params['loss'], learning_rate=best_params['learning_rate'],n_estimators=best_params[\"n_estimators\"], max_depth=best_params[\"max_depth\"],min_samples_leaf=best_params['min_samples_leaf'], min_samples_split=best_params['min_samples_split'])\n",
    "        # return SVR()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here the function is choosing the data set to be used. Again those we are not using are commented out."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "def choose_dataset():\n",
    "    return 'Cyclic'\n",
    "    # return 'Acyclic'\n",
    "    # return 'Combined'\n",
    "    # return 'Cyclohexenone'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function is for tuning the hyper parameters, if we call the function to do so."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "def hyperparam_tune(X, y, model):\n",
    "    print(str(model))\n",
    "    if str(model) == 'RandomForestRegressor()':\n",
    "        hyperP = dict(n_estimators=[100, 300, 500, 800], max_depth=[None, 5, 8, 15, 25, 30],\n",
    "                      min_samples_split=[2, 5, 10, 15, 100],\n",
    "                      min_samples_leaf=[1, 2, 5, 10])\n",
    "\n",
    "    elif str(model) == 'GradientBoostingRegressor()':\n",
    "        hyperP = dict(loss=['ls'], learning_rate=[0.1, 0.2, 0.3],\n",
    "                      n_estimators=[100, 300, 500, 800], max_depth=[None, 5, 8, 15, 25, 30],\n",
    "                      min_samples_split=[2],\n",
    "                      min_samples_leaf=[1, 2])\n",
    "\n",
    "    elif str(model) == 'SVR()':\n",
    "        hyperP = dict(kernel=['linear', 'rbf', 'poly', 'sigmoid'], gamma=['scale', 'auto'], C=[0.1, 1, 5, 10],\n",
    "                      epsilon=[0.001, 0.01, 0.1, 1, 5])\n",
    "\n",
    "\n",
    "    gridF = GridSearchCV(model, hyperP, cv=3, verbose=1, n_jobs=-1)\n",
    "    bestP = gridF.fit(X, y)\n",
    "    print(bestP.best_params_)\n",
    "    return bestP.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we define the random seeds.\n",
    "\n",
    "Computers are pseudo random, so we need to initialise our random number generators"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[831 875 107 124 806 526 107 768 212 695 689 575 822 576 776  58 293 262\n",
      " 663 277  83 923 181 939 454 932 795 628 704  33]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t7/mm1tdtr101z3xh07kxn136xc0000gn/T/ipykernel_17814/299116365.py:1: DeprecationWarning: This function is deprecated. Please call randint(0, 1000 + 1) instead\n",
      "  random_seeds = np.random.random_integers(0, high=1000, size=30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random_seeds = np.random.random_integers(0, high=1000, size=30)\n",
    "print(random_seeds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the 'list' of descriptor names. Basically the headings from our CSV file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The CSV file is read in using the pandas read_csv function, and the columns in our file\n",
    "that do not correspond to the descriptors we have defined are removed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "descriptors = ['LVR1', 'LVR2', 'LVR3', 'LVR4', 'LVR5', 'LVR6', 'LVR7', 'VB', 'ER1', 'ER2', 'ER3', 'ER4', 'ER5', 'ER6',\n",
    "               'ER7', 'SStoutR1', 'SStoutR2', 'SStoutR3', 'SStoutR4', '%top']\n",
    "\n",
    "data = pd.read_csv(choose_dataset()+'.csv')\n",
    "\n",
    "data = data.filter(descriptors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the data read in we also remove those rows containing any erroneous values\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           LVR1  LVR2       LVR3       LVR4       LVR5       LVR6       LVR7  \\\n",
      "0     94.184290   0.0   0.000000  94.184290   0.000000   0.000000  20.255985   \n",
      "1     94.184290   0.0   0.000000  94.184290   0.000000   0.000000  20.255985   \n",
      "2     53.529726   0.0   0.000000   0.000000  21.574215   0.000000  76.074168   \n",
      "3     36.233741   0.0   0.000000  73.462170  21.574215   0.000000  76.074168   \n",
      "4     56.166185   0.0   0.000000  76.888305  21.574215   0.000000  76.074168   \n",
      "..          ...   ...        ...        ...        ...        ...        ...   \n",
      "159    0.000000   0.0  95.697682   0.000000   0.000000  95.697682  20.255985   \n",
      "160  168.732323   0.0   0.000000   0.000000   0.000000   0.000000  32.691970   \n",
      "161   94.184290   0.0   0.000000  94.184290   0.000000   0.000000  20.255985   \n",
      "162   64.956412   0.0   0.000000  21.574215   0.000000   0.000000  84.579926   \n",
      "163   64.956412   0.0   0.000000  21.574215   0.000000   0.000000  84.579926   \n",
      "\n",
      "             VB   ER1  ER2   ER3   ER4   ER5   ER6   ER7   SStoutR1  \\\n",
      "0    348.031937 -0.09  0.0  0.00 -0.09  0.00  0.00  0.00  22.511984   \n",
      "1    134.623998 -0.09  0.0  0.00 -0.09  0.00  0.00  0.00  22.511984   \n",
      "2    134.623998 -0.14  0.0  0.00  0.00 -0.17  0.00 -0.27  22.511984   \n",
      "3    134.623998  0.07  0.0  0.00 -0.12 -0.17  0.00 -0.27  22.511984   \n",
      "4    134.623998 -0.13  0.0  0.00 -0.01 -0.17  0.00 -0.27  22.511984   \n",
      "..          ...   ...  ...   ...   ...   ...   ...   ...        ...   \n",
      "159  134.623998  0.00  0.0 -0.15  0.00  0.00 -0.15  0.00  22.511984   \n",
      "160  163.153375  0.00  0.0  0.00  0.00  0.00  0.00  0.00  22.511984   \n",
      "161  180.449360 -0.09  0.0  0.00 -0.09  0.00  0.00  0.00  22.511984   \n",
      "162  180.449360  0.00  0.0  0.00 -0.17  0.00  0.00 -0.15  22.511984   \n",
      "163  166.579510  0.00  0.0  0.00 -0.17  0.00  0.00 -0.15  22.511984   \n",
      "\n",
      "      SStoutR2  SStoutR3  SStoutR4       %top  \n",
      "0    21.574215         0         0   6.000000  \n",
      "1    19.435100         0         0   2.333333  \n",
      "2    19.435100         0         0  79.000000  \n",
      "3    19.435100         0         0  97.500000  \n",
      "4    19.435100         0         0  91.000000  \n",
      "..         ...       ...       ...        ...  \n",
      "159  21.574215         0         0  91.000000  \n",
      "160  19.435100         0         0   4.000000  \n",
      "161  19.435100         0         0   2.000000  \n",
      "162  19.435100         0         0   0.300000  \n",
      "163  19.435100         0         0   0.300000  \n",
      "\n",
      "[164 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "#remove erroneous data\n",
    "data = data.dropna(axis=0)\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We split our data into target  'Y', and descriptors 'X'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18363981  0.          0.         ...  1.          0.\n",
      "   0.        ]\n",
      " [ 0.18363981  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.24800929  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.18363981  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.12668666  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.12668666  0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "0       6.000000\n",
      "1       2.333333\n",
      "2      79.000000\n",
      "3      97.500000\n",
      "4      91.000000\n",
      "         ...    \n",
      "159    91.000000\n",
      "160     4.000000\n",
      "161     2.000000\n",
      "162     0.300000\n",
      "163     0.300000\n",
      "Name: %top, Length: 164, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = data.drop(['%top'], axis = 1)\n",
    "X = RobustScaler().fit_transform(np.array(X))\n",
    "y = data['%top']\n",
    "print(X)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we have predefined our best parameters as a 'dictionary'.\n",
    "The parameter searching function usage is commented out.\n",
    "\n",
    "We also define containers for storing the results of our training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "#best_params = hyperparam_tune(X, y, choose_model(best_params=None)) #hyperparameter tuning completed on whole subset\n",
    "best_params = {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 500}\n",
    "r2_cv_scores = []\n",
    "rmse_cv_scores = []\n",
    "r2_val_scores = []\n",
    "rmse_val_scores = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the models now all set up and ready to run we can start the training\n",
    "\n",
    "We loop over our random seeds, as we want to try different starting positions\n",
    "in the search space for fitting the model.\n",
    "\n",
    "Each time the data is split randomly into test and validation data to prevent overfitting.\n",
    "\n",
    "K fold crossvalidation fitting is also performed to further ensure no over fitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mEmpty\u001B[0m                                     Traceback (most recent call last)",
      "\u001B[0;32m~/.conda/envs/IntroChemPython/lib/python3.8/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36mdispatch_one_batch\u001B[0;34m(self, iterator)\u001B[0m\n\u001B[1;32m    819\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 820\u001B[0;31m                 \u001B[0mtasks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ready_batches\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mblock\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    821\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mqueue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEmpty\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/IntroChemPython/lib/python3.8/queue.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    166\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_qsize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 167\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mEmpty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    168\u001B[0m             \u001B[0;32melif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mEmpty\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/t7/mm1tdtr101z3xh07kxn136xc0000gn/T/ipykernel_17814/4019005095.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mkfold\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m             \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mchoose_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbest_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m             \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m             \u001B[0mpredictions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtest\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/IntroChemPython/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    385\u001B[0m             \u001B[0;31m# parallel_backend contexts set at a higher level,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    386\u001B[0m             \u001B[0;31m# since correctness does not rely on using threads.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 387\u001B[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001B[0m\u001B[1;32m    388\u001B[0m                              \u001B[0;34m**\u001B[0m\u001B[0m_joblib_parallel_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprefer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'threads'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    389\u001B[0m                 delayed(_parallel_build_trees)(\n",
      "\u001B[0;32m~/.conda/envs/IntroChemPython/lib/python3.8/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1042\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_iterating\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_original_iterator\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1043\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1044\u001B[0;31m             \u001B[0;32mwhile\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdispatch_one_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1045\u001B[0m                 \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1046\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/IntroChemPython/lib/python3.8/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36mdispatch_one_batch\u001B[0;34m(self, iterator)\u001B[0m\n\u001B[1;32m    846\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mislice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfinal_batch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    847\u001B[0m                     tasks = BatchedCalls(islice[i:i + final_batch_size],\n\u001B[0;32m--> 848\u001B[0;31m                                          \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_nested_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    849\u001B[0m                                          \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reducer_callback\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    850\u001B[0m                                          self._pickle_cache)\n",
      "\u001B[0;32m~/.conda/envs/IntroChemPython/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001B[0m in \u001B[0;36mget_nested_backend\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    213\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_nested_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    214\u001B[0m         \u001B[0;31m# import is not top level to avoid cyclic import errors.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 215\u001B[0;31m         \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mparallel\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mget_active_backend\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    216\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    217\u001B[0m         \u001B[0;31m# SequentialBackend should neither change the nesting level, the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/IntroChemPython/lib/python3.8/importlib/_bootstrap.py\u001B[0m in \u001B[0;36mparent\u001B[0;34m(self)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(random_seeds)):\n",
    "    # split into training and validation sets, 9:1\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                  test_size=0.1, random_state=random_seeds[i])\n",
    "    X_train = np.array(X_train).astype('float64')\n",
    "    X_val = np.array(X_val).astype('float64')\n",
    "    y_train = np.array(y_train).astype('float64')\n",
    "    y_val = np.array(y_val).astype('float64')\n",
    "\n",
    "    # 5 fold CV on training set, repeated 3 times\n",
    "    for j in range(3):\n",
    "        kfold = balancedkfold.BalancedKFold(5, verbose=False)  # Kohavi, 1995, stratified KFold\n",
    "        for train, test in kfold.split(X_train, y_train):\n",
    "            model = choose_model(best_params)\n",
    "            model.fit(X_train[train], y_train[train])\n",
    "            predictions = model.predict(X_train[test]).reshape(1, -1)[0]\n",
    "\n",
    "            r2 = r2_score(y_train[test], predictions)\n",
    "            rmse = math.sqrt(mean_squared_error(predictions, y_train[test]))\n",
    "            r2_cv_scores.append(r2)\n",
    "            rmse_cv_scores.append(rmse)\n",
    "\n",
    "\n",
    "    # predict on validaiton set\n",
    "    model = choose_model(best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_val)\n",
    "    r2 = r2_score(y_val, predictions)\n",
    "    rmse = math.sqrt(mean_squared_error(predictions, y_val))\n",
    "    r2_val_scores.append(r2)\n",
    "    rmse_val_scores.append(rmse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The final model results are then return to the command line."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print('Model:',  model)\n",
    "print('Data Subset: ',  choose_dataset())\n",
    "print('Random Seeds: ', random_seeds, '\\n')\n",
    "print('Num CV Scores: ', len(r2_cv_scores))\n",
    "print('CV R2 Mean: ', np.mean(np.array(r2_cv_scores)), '+/-', np.std(np.array(r2_cv_scores)))\n",
    "print('CV RMSE Mean %: ', np.mean(np.array(rmse_cv_scores)), '+/-', np.std(np.array(rmse_cv_scores)), '\\n')\n",
    "print('Num Val Scores: ', len(r2_val_scores))\n",
    "print(r2_val_scores)\n",
    "print('Val r2 Mean: ', np.mean(np.array(r2_val_scores)), '+/-', np.std(np.array(r2_val_scores)))\n",
    "print('Val RMSE Mean %: ', np.mean(np.array(rmse_val_scores)), '+/-', np.std(np.array(rmse_val_scores)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}